{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ignaziogallo/data-mining/blob/aa20-21/tutorials/data/BART.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BERT\n",
    "\n",
    "* BERT is a **deep learning model** that has given state-of-the-art results on a wide variety of natural language \n",
    "* BERT is a recent addition to NLP **pre-training** techniques; \n",
    "* The best part about BERT is that it can be download and used **for free**\n",
    "  * we can either use the  BERT models to extract high quality language features from our text data, \n",
    "  * or we can **fine-tune** these models on a specific task, like sentiment analysis and question answering, with our own data to produce state-of-the-art predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the core idea behind it?\n",
    "\n",
    "* The task is to “`fill in the blank`” based on context. \n",
    "* For example, given\n",
    ">“The woman went to the store and bought a ________________ of shoes.”\n",
    "\n",
    "* a language model might complete this sentence by saying that \n",
    "  * the word “`cart`” would fill the blank 20% of the time \n",
    "  * and the word “`pair`” 80% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* BERT is **bidirectionally trained** (this is also its key technical innovation). \n",
    "  * This means we can now have a deeper sense of language context\n",
    "* Instead of predicting the next word in a sequence, BERT makes use of a novel technique called **Masked LM** (MLM): \n",
    "  * it randomly masks words in the sentence and then it tries to predict them. \n",
    "  * Masking means that the model looks in **both directions** and it uses the full context of the sentence, both left and right surroundings, in order to predict the masked word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BERT architecture\n",
    "For an in-depth understanding of the building blocks of BERT, check [this post](http://jalammar.github.io/illustrated-transformer/).\n",
    "<img src=\"figures/BERT-model.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* [https://www.kaggle.com/abhinand05/bert-for-humans-tutorial-baseline](https://www.kaggle.com/abhinand05/bert-for-humans-tutorial-baseline)\n",
    "* [https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
